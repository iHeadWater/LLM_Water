{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bcb637-3710-4cf6-bd04-dd046965cad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('44611/user/xushuolong1'), PosixPath('//127.0.0.1')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/user/xushuolong1')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/user/xushuolong1/oauth_callback')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"access'), PosixPath('servers!server=xushuolong1/\", \"access'), PosixPath('servers!user=xushuolong1\"]')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8081/hub/api'), PosixPath('//127.0.0.1')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//127.0.0.1'), PosixPath('8081/hub/api/users/xushuolong1/activity')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('WaterGPT-master/Untitled.ipynb')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/xushuolong1/.conda/envs/openai_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "INFO  2023-06-02 17:39:02,658-1d: \n",
      "loading model config\n",
      "llm device: cuda\n",
      "embedding device: cuda\n",
      "dir: /home/xushuolong1/WaterGPT-master\n",
      "flagging username: 957c707dcf494d6694e2a70ad63760c3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers \n",
    "import models.shared as shared \n",
    "from langchain.llms.base import LLM\n",
    "from transformers.generation.logits_process import LogitsProcessor\n",
    "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList\n",
    "from typing import Optional, List, Dict, Any\n",
    "from models.loader import LoaderCheckPoint\n",
    "from models.extensions.callback import (Iteratorize, Stream, FixedLengthQueue) \n",
    "from langchain.callbacks.manager import (\n",
    "    CallbackManagerForLLMRun\n",
    ")\n",
    "from abc import ABC\n",
    "from configs.model_config import *\n",
    "import random\n",
    "from models.base import (BaseAnswer,\n",
    "                         AnswerResult,\n",
    "                         AnswerResultStream,\n",
    "                         AnswerResultQueueSentinelTokenListenerQueue)\n",
    "from langchain.callbacks.manager import (\n",
    "    CallbackManagerForLLMRun\n",
    ")\n",
    "import asyncio\n",
    "from argparse import Namespace\n",
    "from models.loader.args import parser\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from typing import List, Set, Union\n",
    "from langchain import SerpAPIWrapper\n",
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe88115-dd37-4737-bac6-16522f0b60f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_remote_model': True, 'model': 'chatglm-6b', 'lora': None, 'model_dir': '/data/ssd/xsl/models/', 'lora_dir': 'loras/', 'cpu': False, 'auto_devices': False, 'gpu_memory': None, 'cpu_memory': None, 'load_in_8bit': False, 'bf16': True}\n",
      "Loading chatglm-6b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e2f0b8d2f14954aebd0bffddf2f061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 53.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=['--model-dir', '/data/ssd/xsl/models/',  '--model', 'chatglm-6b', '--no-remote-model', '--bf16'])\n",
    "\n",
    "args_dict = vars(args)\n",
    "print(args_dict)\n",
    "shared.loaderCheckPoint = LoaderCheckPoint(args_dict)\n",
    "torch.cuda.empty_cache()\n",
    "shared.loaderCheckPoint.unload_model()\n",
    "shared.loaderCheckPoint.reload_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d274d575-98e1-444e-8644-5a3e68392a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _streaming_response_template() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    :return: 响应结构\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"text\": \"\"\n",
    "    }\n",
    "\n",
    "\n",
    "def _update_response(response: Dict[str, Any], stream_response: str) -> None:\n",
    "    \"\"\"Update response from the stream response.\"\"\"\n",
    "    response[\"text\"] += stream_response\n",
    "\n",
    "\n",
    "class InvalidScoreLogitsProcessor(LogitsProcessor):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        if torch.isnan(scores).any() or torch.isinf(scores).any():\n",
    "            scores.zero_()\n",
    "            scores[..., 5] = 5e4\n",
    "        return scores\n",
    "\n",
    "\n",
    "class LLamaLLM(BaseAnswer, LLM, ABC):\n",
    "    checkPoint: LoaderCheckPoint = None\n",
    "    history = []\n",
    "    history_len: int = 3\n",
    "    max_new_tokens: int = 500\n",
    "    num_beams: int = 1\n",
    "    temperature: float = 0\n",
    "    top_p: float = 0.4\n",
    "    top_k: int = 10\n",
    "    repetition_penalty: float = 1.12\n",
    "    encoder_repetition_penalty: int = 1\n",
    "    min_length: int = 0\n",
    "    logits_processor: LogitsProcessorList = None\n",
    "    stopping_criteria: Optional[StoppingCriteriaList] = None\n",
    "\n",
    "    state: object = {'max_new_tokens': 50,\n",
    "                     'seed': 1,\n",
    "                     'temperature': 0, 'top_p': 0.1,\n",
    "                     'top_k': 40, 'typical_p': 1,\n",
    "                     'repetition_penalty': 1.18,\n",
    "                     'encoder_repetition_penalty': 1,\n",
    "                     'no_repeat_ngram_size': 0,\n",
    "                     'min_length': 0,\n",
    "                     'penalty_alpha': 0,\n",
    "                     'num_beams': 1,\n",
    "                     'length_penalty': 1,\n",
    "                     'early_stopping': False, 'add_bos_token': True, 'ban_eos_token': False,\n",
    "                     'truncation_length': 2048, 'custom_stopping_strings': '',\n",
    "                     'cpu_memory': 0, 'auto_devices': False, 'disk': False, 'cpu': False, 'bf16': False,\n",
    "                     'load_in_8bit': False, 'wbits': 'None', 'groupsize': 'None', 'model_type': 'None',\n",
    "                     'pre_layer': 0, 'gpu_memory_0': 0}\n",
    "\n",
    "    def __init__(self, checkPoint: LoaderCheckPoint = None):\n",
    "        super().__init__()\n",
    "        self.checkPoint = checkPoint\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"LLamaLLM\"\n",
    "\n",
    "    @property\n",
    "    def _check_point(self) -> LoaderCheckPoint:\n",
    "        return self.checkPoint\n",
    "\n",
    "    def encode(self, prompt, add_special_tokens=True, add_bos_token=True, truncation_length=None):\n",
    "        input_ids = self.checkPoint.tokenizer.encode(str(prompt), return_tensors='pt',\n",
    "                                                     add_special_tokens=add_special_tokens)\n",
    "        # This is a hack for making replies more creative.\n",
    "        if not add_bos_token and input_ids[0][0] == self.checkPoint.tokenizer.bos_token_id:\n",
    "            input_ids = input_ids[:, 1:]\n",
    "\n",
    "        # Llama adds this extra token when the first character is '\\n', and this\n",
    "        # compromises the stopping criteria, so we just remove it\n",
    "        if type(self.checkPoint.tokenizer) is transformers.LlamaTokenizer and input_ids[0][0] == 29871:\n",
    "            input_ids = input_ids[:, 1:]\n",
    "\n",
    "        # Handling truncation\n",
    "        if truncation_length is not None:\n",
    "            input_ids = input_ids[:, -truncation_length:]\n",
    "\n",
    "        return input_ids.cuda('cuda:1')\n",
    "\n",
    "    def decode(self, output_ids):\n",
    "        reply = self.checkPoint.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        return reply\n",
    "\n",
    "    def generate_with_callback(self, callback=None, **kwargs):\n",
    "        self.checkPoint.clear_torch_cache()\n",
    "        kwargs['stopping_criteria'].append(Stream(callback_func=callback))\n",
    "        with torch.no_grad():\n",
    "            self.checkPoint.model.generate(**kwargs)\n",
    "            print(\"方法结束\")\n",
    "\n",
    "    def generate_with_streaming(self, **kwargs):\n",
    "        return Iteratorize(self.generate_with_callback, kwargs)\n",
    "\n",
    "    # 将历史对话数组转换为文本格式\n",
    "    def history_to_text(self, query):\n",
    "        formatted_history = ''\n",
    "        history = self.history[-self.history_len:] if self.history_len > 0 else []\n",
    "        for i, (old_query, response) in enumerate(history):\n",
    "            formatted_history += \"[Round {}]\\n问：{}\\n答：{}\\n\".format(i, old_query, response)\n",
    "        formatted_history += \"[Round {}]\\n问：{}\\n答：\".format(len(history), query)\n",
    "        return formatted_history\n",
    "\n",
    "    def prepare_inputs_for_generation(self,\n",
    "                                      input_ids: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        预生成注意力掩码和 输入序列中每个位置的索引的张量\n",
    "        # TODO 没有思路\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        mask_positions = torch.zeros((1, input_ids.shape[1]), dtype=input_ids.dtype).to(self.checkPoint.model.device)\n",
    "\n",
    "        attention_mask = self.get_masks(input_ids, input_ids.device)\n",
    "\n",
    "        position_ids = self.get_position_ids(\n",
    "            input_ids,\n",
    "            device=input_ids.device,\n",
    "            mask_positions=mask_positions\n",
    "        )\n",
    "\n",
    "        return input_ids, position_ids, attention_mask\n",
    "\n",
    "    def get_position_ids(self, input_ids: torch.LongTensor, mask_positions, device):\n",
    "        \"\"\"\n",
    "        注意力偏移量\n",
    "        :param input_ids:\n",
    "        :param mask_positions:\n",
    "        :param device:\n",
    "        :param use_gmasks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        context_lengths = [seq.tolist().index(self.checkPoint.model_config.bos_token_id) for seq in input_ids]\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        for i, context_length in enumerate(context_lengths):\n",
    "            position_ids[i, context_length:] = mask_positions[i]\n",
    "        block_position_ids = [torch.cat((\n",
    "            torch.zeros(context_length, dtype=torch.long, device=device),\n",
    "            torch.arange(seq_length - context_length, dtype=torch.long, device=device) + 1\n",
    "        )) for context_length in context_lengths]\n",
    "        block_position_ids = torch.stack(block_position_ids, dim=0)\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=1)\n",
    "        return position_ids\n",
    "\n",
    "    def get_masks(self, input_ids, device):\n",
    "        \"\"\"\n",
    "        获取注意力掩码\n",
    "        :param input_ids:\n",
    "        :param device:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        context_lengths = [seq.tolist().index(self.checkPoint.model_config.bos_token_id) for seq in input_ids]\n",
    "        attention_mask = torch.ones((batch_size, seq_length, seq_length), device=device)\n",
    "        attention_mask.tril_()\n",
    "        for i, context_length in enumerate(context_lengths):\n",
    "            attention_mask[i, :, :context_length] = 1\n",
    "        attention_mask.unsqueeze_(1)\n",
    "        attention_mask = (attention_mask < 0.5).bool()\n",
    "        return attention_mask\n",
    "\n",
    "    def generate_softprompt_history_tensors(self, query):\n",
    "        \"\"\"\n",
    "        历史对话软提示\n",
    "            这段代码首先定义了一个名为 history_to_text 的函数，用于将 self.history\n",
    "            数组转换为所需的文本格式。然后，我们将格式化后的历史文本\n",
    "            再用 self.encode 将其转换为向量表示。最后，将历史对话向量与当前输入的对话向量拼接在一起。\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 对话内容\n",
    "        # 处理历史对话\n",
    "        formatted_history = self.history_to_text(query)\n",
    "        return formatted_history\n",
    "\n",
    "    def _call(self,\n",
    "              prompt: str,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
    "        print(f\"__call:{prompt}\")\n",
    "        if self.logits_processor is None:\n",
    "            self.logits_processor = LogitsProcessorList()\n",
    "        self.logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"num_beams\": self.num_beams,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"repetition_penalty\": self.repetition_penalty,\n",
    "            \"encoder_repetition_penalty\": self.encoder_repetition_penalty,\n",
    "            \"min_length\": self.min_length,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"logits_processor\": self.logits_processor}\n",
    "\n",
    "        #  向量拼接\n",
    "        input_ids = self.encode(prompt, add_bos_token=self.state['add_bos_token'], truncation_length=self.max_new_tokens)\n",
    "        # input_ids, position_ids, attention_mask = self.prepare_inputs_for_generation(input_ids=filler_input_ids)\n",
    "\n",
    "        # 对话模型prompt\n",
    "        gen_kwargs.update({'inputs': input_ids})\n",
    "        # 注意力掩码\n",
    "        # gen_kwargs.update({'attention_mask': attention_mask})\n",
    "        # gen_kwargs.update({'position_ids': position_ids})\n",
    "        if self.stopping_criteria is None:\n",
    "            self.stopping_criteria = transformers.StoppingCriteriaList()\n",
    "        # 观测输出\n",
    "        gen_kwargs.update({'stopping_criteria': self.stopping_criteria})\n",
    "        shared.stop_everything = False\n",
    "        stopped = False\n",
    "        response_template = _streaming_response_template()\n",
    "\n",
    "        # TODO 此流输出方法需要重写！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "        # stopping_criteria方法不可控制 迭代器的变量无法共享\n",
    "        with self.generate_with_streaming(**gen_kwargs) as generator:\n",
    "            last_reply_len = 0\n",
    "            reply_index = 0\n",
    "            # Create a FixedLengthQueue with the desired stop sequence and a maximum length.\n",
    "            queue = FixedLengthQueue(stop)\n",
    "            for output in generator:\n",
    "                new_tokens = len(output) - len(input_ids[0])\n",
    "                reply = self.decode(output[-new_tokens:])\n",
    "\n",
    "                new_reply = len(reply) - last_reply_len\n",
    "                output_reply = reply[-new_reply:]\n",
    "                queue.add(reply_index, output_reply)\n",
    "                queue.contains_replace_sequence()\n",
    "                if stop:\n",
    "                    pos = queue.contains_stop_sequence()\n",
    "                    if pos != -1:\n",
    "                        shared.stop_everything = True\n",
    "                        stopped = True\n",
    "\n",
    "                #print(f\"{reply_index}：reply  {output_reply}\")\n",
    "                english_reply = queue.put_replace_out(reply_index)\n",
    "                #print(f\"{reply_index}：english_reply  {english_reply}\")\n",
    "                _update_response(response_template, english_reply)\n",
    "                last_reply_len = len(reply)\n",
    "\n",
    "                reply_index += 1\n",
    "                if new_tokens == self.max_new_tokens - 1 or stopped:\n",
    "                    break\n",
    "\n",
    "        response = response_template['text']\n",
    "        print(f\"response:{response}\")\n",
    "        self.history = self.history + [[None, response]]\n",
    "        return response\n",
    "\n",
    "    def _generate_answer(self, prompt: str,\n",
    "                         history: List[List[str]] = [],\n",
    "                         streaming: bool = False,\n",
    "                         generate_with_callback: AnswerResultStream = None) -> None:\n",
    "        if history:\n",
    "            self.history = history\n",
    "        # Create the StoppingCriteriaList with the stopping strings\n",
    "        self.stopping_criteria = transformers.StoppingCriteriaList()\n",
    "        # 定义模型stopping_criteria 队列，在每次响应时将 torch.LongTensor, torch.FloatTensor同步到AnswerResult\n",
    "        listenerQueue = AnswerResultQueueSentinelTokenListenerQueue()\n",
    "        self.stopping_criteria.append(listenerQueue)\n",
    "        # TODO 需要实现chat对话模块和注意力模型，目前_call为langchain的LLM拓展的api，默认为无提示词模式，如果需要操作注意力模型，可以参考chat_glm的实现\n",
    "        softprompt = self.generate_softprompt_history_tensors(prompt)\n",
    "        response = self._call(prompt=softprompt, stop=['\\n###'])\n",
    "        answer_result = AnswerResult()\n",
    "        answer_result.history = self.history\n",
    "        if listenerQueue.listenerQueue.__len__() > 0:\n",
    "            answer_result.listenerToken = listenerQueue.listenerQueue.pop()\n",
    "        answer_result.llm_output = {\"answer\": response}\n",
    "        generate_with_callback(answer_result)\n",
    "        \n",
    "    def _history_len():\n",
    "        return LLM_HISTORY_LEN\n",
    "        \n",
    "    def set_history_len():\n",
    "        return LLM_HISTORY_LEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1341df-c23c-419e-98d7-f55ee9bfe0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model_ins = LLamaLLM(checkPoint=shared.loaderCheckPoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403dac1b-9f0d-466f-8496-d3cfdf350137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template = \"\"\"This is a conversation between a human and a bot:\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Answer the question for {input}\n",
    "\"\"\"\n",
    "# Write a summary of the conversation for {input}:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"chat_history\"],\n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "summry_chain = LLMChain(\n",
    "    llm=llm_model_ins,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=readonlymemory,  # use the read-only memory to prevent the tool from modifying the memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2eb206-dcf8-490f-9964-d11501f24d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLLMSingleActionAgent(ZeroShotAgent):\n",
    "    allowed_tools: List[str]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomLLMSingleActionAgent, self).__init__(*args, **kwargs)\n",
    "        self.allowed_tools = kwargs['allowed_tools']\n",
    "\n",
    "    def get_allowed_tools(self) -> Set[str]:\n",
    "        return set(self.allowed_tools)\n",
    "\n",
    "# Define which tools the agent can use to answer user queries\n",
    "tools = [ \n",
    "        Tool(\n",
    "            name=\"Football engine\",\n",
    "            func=lambda x: \"Premier Champion of 2023 is Manchester City.\", \n",
    "            description=\"Use it if the question is about Premier Champion, like 'Who is the champion of Premier League in 2023?'\"\n",
    "    )\n",
    "]\n",
    "\n",
    "'''\n",
    "tools = [\n",
    "   Tool(\n",
    "        name=\"Music Search\",\n",
    "        func=lambda x: \"'All I Want For Christmas Is You' by Mariah Carey.\", #Mock Function\n",
    "        description=\"A Music search engine. Use this more than the normal search if the question is about Music, like 'who is the singer of yesterday?' or 'what is the most popular song in 2022?'\",\n",
    "    )\n",
    "] \n",
    "\n",
    "        Tool(\n",
    "            name=\"Summary\",\n",
    "            func=summry_chain.run,\n",
    "            description=\"useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary.\"\n",
    "        )\n",
    "       \n",
    "# Define which tools the agent can use to answer user queries\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "# prefix = \"\"\"Use 'search engine' to search the leader! Don't lie!\"\"\"\n",
    "suffix = \"\"\"Begin!\n",
    " \n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "tool_names = [tool.name for tool in tools]\n",
    "llm_chain = LLMChain(llm=llm_model_ins, prompt=prompt)\n",
    "# print(tool_names)\n",
    "# agent = CustomLLMSingleActionAgent(llm_chain=llm_chain, tools=tools, allowed_tools=tool_names)\n",
    "# agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e068dc-2b2f-4841-9036-ab97edf3c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94a0336-2b22-41d4-8b52-e1c65c9cb013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3f5e33-1535-47af-aafb-959d99ea8a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0179c374-5b67-4b4f-b06a-38dbb74267db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "__call:Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Football engine: Use it if the question is about Premier Champion, like 'Who is the champion of Premier League in 2023?'\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Football engine]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      " \n",
      "Question: Who is the champion of Premier League in 2023?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 2023-06-02 17:41:17,387-1d: The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法结束\n",
      "invalid thread id\n",
      "response:The champion of the Premier League in 2023 is Manchester City.The champion of the Premier League in 2023 is Manchester City.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse LLM output: `The champion of the Premier League in 2023 is Manchester City.The champion of the Premier League in 2023 is Manchester City.`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is the champion of Premier League in 2023?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai_env/lib/python3.10/site-packages/langchain/chains/base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.conda/envs/openai_env/lib/python3.10/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.conda/envs/openai_env/lib/python3.10/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.conda/envs/openai_env/lib/python3.10/site-packages/langchain/agents/agent.py:947\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 947\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m    956\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    957\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/openai_env/lib/python3.10/site-packages/langchain/agents/agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/openai_env/lib/python3.10/site-packages/langchain/agents/agent.py:345\u001b[0m, in \u001b[0;36mLLMSingleActionAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    340\u001b[0m     intermediate_steps\u001b[38;5;241m=\u001b[39mintermediate_steps,\n\u001b[1;32m    341\u001b[0m     stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop,\n\u001b[1;32m    342\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m )\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mCustomOutputParser.parse\u001b[0;34m(self, llm_output)\u001b[0m\n\u001b[1;32m     21\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(regex, llm_output, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m action \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     25\u001b[0m action_input \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse LLM output: `The champion of the Premier League in 2023 is Manchester City.The champion of the Premier League in 2023 is Manchester City.`"
     ]
    }
   ],
   "source": [
    "agent_executor.run(\"Who is the champion of Premier League in 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ad866-7fc6-494d-bc00-5c391f6d441c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "openai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
